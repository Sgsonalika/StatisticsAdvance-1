{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the properties of the F-distribution.\n",
        "\n",
        "Ans:\n",
        "\n",
        "The F-distribution is a continuous probability distribution that arises frequently in the context of variance analysis, especially in ANOVA and F-tests. It has several important properties:\n",
        "\n",
        "1. Non-negative values only: The F-distribution is defined only for non-negative values (i.e., it is right-skewed and starts at 0).\n",
        "\n",
        "2. Asymmetrical / Right-skewed: It is positively skewed (i.e., long right tail), especially when the degrees of freedom are small.\n",
        "\n",
        "3. Shape depends on degrees of freedom: The F-distribution has two parameters ‚Äî degrees of freedom for the numerator (df1) and the denominator (df2). Its shape varies based on these.\n",
        "\n",
        "Mean and variance:\n",
        "\n",
        "Mean =\n",
        "ùëë\n",
        "ùëì\n",
        "2\n",
        "ùëë\n",
        "ùëì\n",
        "2\n",
        "‚àí\n",
        "2\n",
        "df\n",
        "2\n",
        "‚Äã\n",
        " ‚àí2\n",
        "df\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "  for\n",
        "ùëë\n",
        "ùëì\n",
        "2\n",
        ">\n",
        "2\n",
        "df\n",
        "2\n",
        "‚Äã\n",
        " >2\n",
        "\n",
        "Variance is defined only when\n",
        "ùëë\n",
        "ùëì\n",
        "2\n",
        ">\n",
        "4\n",
        "df\n",
        "2\n",
        "‚Äã\n",
        " >4\n",
        "\n",
        "4. Not symmetric: Unlike the normal distribution, the F-distribution is not symmetric, and as degrees of freedom increase, it becomes more bell-shaped but still skewed.\n",
        "\n"
      ],
      "metadata": {
        "id": "XIGkmGQKlrk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "Ans:\n",
        "\n",
        "The F-distribution is primarily used in the following statistical tests:\n",
        "\n",
        "1. ANOVA (Analysis of Variance):\n",
        "Used to compare the means of three or more groups. The F-statistic is the ratio of the variance between the group means to the variance within the groups.\n",
        "\n",
        "2. F-test for equality of variances:\n",
        "Used to compare the variances of two populations to determine if they are significantly different.\n",
        "\n",
        "3. Regression analysis (overall model significance):\n",
        "The F-distribution is used to test whether the explained variance in a regression model is significantly greater than the unexplained variance.\n",
        "\n",
        "> Why F-distribution is appropriate:\n",
        "\n",
        "* Because it compares two variances (a ratio), and the F-distribution is derived from the ratio of two independent chi-squared variables divided by their degrees of freedom.\n",
        "\n",
        "* Since variances are always positive, a distribution like F (which is only defined for non-negative values) fits the requirement.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jtli1-6Hnt0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "\n",
        "Ans: To ensure the validity of an F-test for comparing variances, the following assumptions must be met:\n",
        "\n",
        "1. Independence: The two samples must be independent of each other.\n",
        "\n",
        "2. Normality: Both populations should follow a normal distribution. The F-test is highly sensitive to deviations from normality.\n",
        "\n",
        "3. Random Sampling: The data should be collected using a random sampling method.\n",
        "\n",
        "4. Measurement Scale: The variable should be measured on at least an interval scale.\n",
        "\n",
        "Violating the assumption of normality can lead to incorrect conclusions, which is why alternative tests (like Levene‚Äôs test) are sometimes preferred in practice."
      ],
      "metadata": {
        "id": "Lx8aUWycoreN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "Ans:\n",
        "\n",
        "The purpose of ANOVA (Analysis of Variance) is to determine whether there are statistically significant differences between the means of three or more independent groups. It helps to answer the question: Are all group means equal, or is at least one different?\n",
        "\n",
        "ANOVA does this by comparing the variance between groups (how much the group means differ from the overall mean) to the variance within groups (how much individual values differ within each group). If the between-group variance is large relative to the within-group variance, it suggests that not all group means are equal.\n",
        "\n",
        "The t-test, on the other hand, is used when comparing the means of two groups only. It assesses whether the difference between the two sample means is statistically significant.\n",
        "\n",
        "The key difference lies in the number of groups being compared. While a t-test is limited to two, ANOVA allows for the comparison of multiple groups at once. This is important because running multiple t-tests for three or more groups increases the chance of making a Type I error (finding a difference when none exists). ANOVA avoids this problem by providing a single, overall test of significance.\n",
        "\n",
        "In summary, ANOVA is an extension of the t-test designed to handle multiple groups efficiently and with greater control over error rates."
      ],
      "metadata": {
        "id": "0QDHy8nBp-PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "\n",
        "Ans:\n",
        "\n",
        "**When to use one-way ANOVA:**\n",
        "\n",
        "Use a one-way ANOVA when:\n",
        "\n",
        "* You are comparing three or more independent groups.\n",
        "\n",
        "* There is one independent variable (factor) with multiple levels (e.g., three different teaching methods).\n",
        "\n",
        "* The goal is to test whether at least one group mean is significantly different from the others.\n",
        "\n",
        "**Why use ANOVA instead of multiple t-tests:**\n",
        "1. Controls Type I error: Each individual t-test increases the chance of making a Type I error. For example, running 3 t-tests at a 5% significance level gives a higher overall error rate. ANOVA keeps the familywise error rate under control.\n",
        "\n",
        "2. Efficient: ANOVA uses all data simultaneously, making it more powerful and statistically valid.\n",
        "\n",
        "3. Post-hoc testing supported: If ANOVA finds a significant difference, post-hoc tests (like Tukey's HSD) can identify which specific groups differ."
      ],
      "metadata": {
        "id": "Kq0Wl8vEqMUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How is variance partitioned in ANOVA?\n",
        "\n",
        "Ans:\n",
        "\n",
        "In ANOVA (Analysis of Variance), total variance is partitioned into two components:\n",
        "\n",
        "1. Between-group variance (SSB or SSTr):\n",
        "\n",
        "* Measures the variability between the means of different groups.\n",
        "\n",
        "* High between-group variance suggests significant differences between group means.\n",
        "\n",
        "2. Within-group variance (SSW or SSE):\n",
        "\n",
        "* Measures the variability within each group (i.e., how much the values differ within the same group).\n",
        "\n",
        "* This reflects random error or natural variation.\n",
        "\n",
        "**Total Variance = Between-Group Variance + Within-Group Variance**\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "SST¬†(Total¬†Sum¬†of¬†Squares)\n",
        "=\n",
        "SSB¬†(Between)\n",
        "+\n",
        "SSW¬†(Within)\n",
        "SST¬†(Total¬†Sum¬†of¬†Squares)=SSB¬†(Between)+SSW¬†(Within)\n",
        "\n",
        "**F-Statistic Calculation:**\n",
        "\n",
        "\n",
        "ùêπ\n",
        "=\n",
        "Mean¬†Square¬†Between\n",
        "Mean¬†Square¬†Within\n",
        "=\n",
        "SSB/dfB\n",
        "SSW/dfW\n",
        "\n",
        "A larger F-value suggests that group means are significantly different compared to the variation within the groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "pwoKh6Ywqh1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "Ans:\n",
        "\n",
        "\n",
        "In the classical or frequentist approach to ANOVA, we assume that the data is generated from a process with fixed but unknown parameters. We perform calculations like the F-statistic to compare group means and use a p-value to decide whether to reject the null hypothesis. The uncertainty in this approach is handled by the sampling distribution‚Äîthat is, how the test statistic behaves across repeated samples from the population.\n",
        "\n",
        "In contrast, the Bayesian approach to ANOVA treats the parameters themselves (such as the group means and variances) as random variables. It incorporates prior beliefs about these parameters, and then updates those beliefs in light of the data to produce posterior distributions. Instead of computing a p-value, Bayesian methods estimate probabilities directly and may use a Bayes Factor to compare models or hypotheses. This allows researchers to say things like, \"There‚Äôs a 95% probability that the mean of Group A is greater than Group B,\" which is more intuitive than a p-value.\n",
        "\n",
        "Overall, while the frequentist method relies on long-run frequencies and fixed parameter values, the Bayesian method provides a more flexible, probabilistic interpretation of uncertainty and allows the integration of prior knowledge into the analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "JikiBiulreLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n",
        "Ans:  \n",
        "\n",
        "\n",
        "You were given two sets of income data:\n",
        "\n",
        "* Profession A: [48, 52, 55, 60, 62]\n",
        "\n",
        "* Profession B: [45, 50, 55, 52, 47]\n",
        "\n",
        "We used Python to calculate the sample variances:\n",
        "\n",
        "* Variance of Profession A = 32.8\n",
        "\n",
        "* Variance of Profession B = 15.7\n",
        "\n",
        "To compare whether these variances are significantly different, we performed an F-test, which uses the formula:\n",
        "\n",
        "\n",
        "F= larger¬†variance/\n",
        "smaller¬†variance\n",
        "\n",
        "\n",
        " =\n",
        "15.7\n",
        "32.8\n",
        "\n",
        "\n",
        " ‚âà2.09\n",
        "\n",
        "\n",
        "Then we looked up the p-value for this F-statistic with appropriate degrees of freedom (df‚ÇÅ = 4, df‚ÇÇ = 4) and found:\n",
        "\n",
        "* p-value ‚âà 0.493\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Since the p-value is much larger than the common significance level (e.g., 0.05), we fail to reject the null hypothesis. This means that there is no statistically significant difference in the income variances between Profession A and Profession B. In other words, we have no strong evidence that one profession's income is more variable than the other."
      ],
      "metadata": {
        "id": "WFiIlh8YrkyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164]\n",
        "V Region B: [172, 175, 170, 168, 174]\n",
        "V Region C: [180, 182, 179, 185, 183]\n",
        "\n",
        "V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "\n",
        "\n",
        "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "Ans:\n",
        "\n",
        "* Region A: [160, 162, 165, 158, 164]\n",
        "\n",
        "* Region B: [172, 175, 170, 168, 174]\n",
        "\n",
        "* Region C: [180, 182, 179, 185, 183]\n",
        "\n",
        "We used a one-way ANOVA test to determine whether the mean height differs significantly between these three regions.\n",
        "\n",
        "Python output gave us:\n",
        "\n",
        "* F-statistic ‚âà 67.87\n",
        "\n",
        "* p-value ‚âà 2.87 √ó 10‚Åª‚Å∑\n",
        "\n",
        "The F-statistic is a ratio of:\n",
        "\n",
        "* The variance between group means (how different the regions are)\n",
        "\n",
        "* The variance within the groups (how much individuals in the same region differ)\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "The very small p-value indicates that the probability of observing such a large F-statistic due to chance alone is almost zero. Therefore, we reject the null hypothesis and conclude that there is a statistically significant difference in average height between at least two of the regions.\n",
        "\n",
        "This result doesn‚Äôt tell us which regions differ‚Äîonly that at least one group‚Äôs mean height is different. To pinpoint which ones differ, you would follow up with post-hoc tests like Tukey's HSD."
      ],
      "metadata": {
        "id": "3BwmXwqfsDRD"
      }
    }
  ]
}